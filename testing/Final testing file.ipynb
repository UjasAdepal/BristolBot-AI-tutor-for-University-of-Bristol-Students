{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69bd897c",
   "metadata": {},
   "source": [
    "# bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "791dd5c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e0085cd2bbd405b8fa2c8b24527e248",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b19f37f1676b4e2c8ce36ef9c0cc3ada",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "588883ef600047079dc564258279a013",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1c171d8782c4c76af796c1ca61fe143",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d0663660fab4f289495b98e457f614f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbf1410cbab34510a9b2d28a4b0ed549",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f813c5d669e4839800ec9d5e5e8365d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5a2573ee6324bc295ce24ef615e1157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 24.97 seconds, 0.60 sentences/sec\n",
      "Average Precision: 0.9031\n",
      "Average Recall: 0.8713\n",
      "Average F1: 0.8864\n"
     ]
    }
   ],
   "source": [
    "from bert_score import score\n",
    "import json\n",
    "\n",
    "# Load your JSON\n",
    "with open('test_questions.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract AI and GT answers\n",
    "ai_answers = [item['AI'] for item in data]\n",
    "gt_answers = [item['answer'] for item in data]\n",
    "\n",
    "# Calculate BERTScore\n",
    "P, R, F1 = score(ai_answers, gt_answers, lang=\"en\", verbose=True)\n",
    "\n",
    "# Print average\n",
    "print(f\"Average Precision: {P.mean().item():.4f}\")\n",
    "print(f\"Average Recall: {R.mean().item():.4f}\")\n",
    "print(f\"Average F1: {F1.mean().item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed28451c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3602b243",
   "metadata": {},
   "source": [
    "# Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64b86b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROUGE-L F1 Score: 0.2653\n"
     ]
    }
   ],
   "source": [
    "from rouge import Rouge\n",
    "import json\n",
    "\n",
    "# Load your JSON\n",
    "with open('test_questions.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "ai_answers = [item['AI'] for item in data]\n",
    "gt_answers = [item['answer'] for item in data]\n",
    "\n",
    "# Initialize Rouge\n",
    "rouge = Rouge()\n",
    "\n",
    "# Calculate ROUGE-L\n",
    "scores = [rouge.get_scores(ai, gt)[0]['rouge-l']['f'] for ai, gt in zip(ai_answers, gt_answers)]\n",
    "\n",
    "# Print average ROUGE-L score\n",
    "average_rouge_l = sum(scores) / len(scores)\n",
    "print(f\"Average ROUGE-L F1 Score: {average_rouge_l:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee0939b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a29617b",
   "metadata": {},
   "source": [
    "# Bleu and Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b033b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating ROUGE-L...\n",
      "\n",
      "Calculating BLEU Score...\n",
      "\n",
      "--- Evaluation Results ---\n",
      "ROUGE-L F1 Score: 0.2653\n",
      "BLEU Score: 0.0438\n",
      "Average Latency per QA: 0.00 seconds\n",
      "\n",
      "Results saved to evaluation_results.json ✅\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ujasa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Install needed libraries first (uncomment if you haven't installed them)\n",
    "# pip install bert_score\n",
    "# pip install rouge\n",
    "# pip install nltk\n",
    "\n",
    "from bert_score import score as bert_score\n",
    "from rouge import Rouge\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "import json\n",
    "import statistics\n",
    "import time\n",
    "\n",
    "# Download punkt if not already\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Load your JSON data\n",
    "with open('test_questions.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract AI and GT answers\n",
    "ai_answers = [item['AI'] for item in data]\n",
    "gt_answers = [item['answer'] for item in data]\n",
    "\n",
    "# Start timer for latency\n",
    "start_time = time.time()\n",
    "\n",
    "\n",
    "# -------- ROUGE-L --------\n",
    "print(\"\\nCalculating ROUGE-L...\")\n",
    "rouge = Rouge()\n",
    "rouge_l_f1_scores = []\n",
    "for ai, gt in zip(ai_answers, gt_answers):\n",
    "    try:\n",
    "        score = rouge.get_scores(ai, gt)[0]['rouge-l']['f']\n",
    "        rouge_l_f1_scores.append(score)\n",
    "    except ValueError:\n",
    "        # If AI or GT is empty, skip\n",
    "        rouge_l_f1_scores.append(0.0)\n",
    "average_rouge_l_f1 = statistics.mean(rouge_l_f1_scores)\n",
    "\n",
    "# -------- BLEU Score --------\n",
    "print(\"\\nCalculating BLEU Score...\")\n",
    "# Prepare BLEU inputs\n",
    "references = [[gt.split()] for gt in gt_answers]  # Each GT wrapped in a list\n",
    "candidates = [ai.split() for ai in ai_answers]\n",
    "# Smoothing for better BLEU with short sentences\n",
    "smooth_fn = SmoothingFunction().method4\n",
    "bleu_score_value = corpus_bleu(references, candidates, smoothing_function=smooth_fn)\n",
    "\n",
    "# End timer for latency\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "average_latency = total_time / len(data)\n",
    "\n",
    "# -------- Results --------\n",
    "print(\"\\n--- Evaluation Results ---\")\n",
    "\n",
    "print(f\"ROUGE-L F1 Score: {average_rouge_l_f1:.4f}\")\n",
    "print(f\"BLEU Score: {bleu_score_value:.4f}\")\n",
    "print(f\"Average Latency per QA: {average_latency:.2f} seconds\")\n",
    "\n",
    "# Optional: Save results to a file\n",
    "results = {\n",
    "   \n",
    "    \"ROUGE_L_F1\": average_rouge_l_f1,\n",
    "    \"BLEU_Score\": bleu_score_value,\n",
    "    \"Average_Latency_per_QA\": average_latency,\n",
    "    \"Total_QA_Pairs\": len(data)\n",
    "}\n",
    "\n",
    "with open('evaluation_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "print(\"\\nResults saved to evaluation_results.json ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1e767c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24552b80",
   "metadata": {},
   "source": [
    "# cosine similarity and rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f72c0b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.3345\n",
      "ROUGE Scores: {'rouge1': Score(precision=0.5, recall=0.2682926829268293, fmeasure=0.34920634920634924), 'rouge2': Score(precision=0.2857142857142857, recall=0.15, fmeasure=0.1967213114754098), 'rougeL': Score(precision=0.4090909090909091, recall=0.21951219512195122, fmeasure=0.28571428571428575)}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the data from x.json file\n",
    "with open('test_questions.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Assuming that the data is a list and we need to access the first item in it\n",
    "entry = data[0]  # Accessing the first dictionary in the list\n",
    "\n",
    "# Extracting the real and AI answers from the first entry\n",
    "real_answer = entry[\"answer\"]\n",
    "ai_answer = entry[\"AI\"]\n",
    "\n",
    "# Vectorizing the text using TF-IDF\n",
    "vectorizer = TfidfVectorizer().fit_transform([real_answer, ai_answer])\n",
    "cosine_sim = cosine_similarity(vectorizer[0:1], vectorizer[1:2])\n",
    "\n",
    "# ROUGE score (recall-based metric)\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "rouge_scores = scorer.score(real_answer, ai_answer)\n",
    "\n",
    "# Word Overlap / Exact Match (counting matching words)\n",
    "exact_match = len(set(real_answer.lower().split()).intersection(set(ai_answer.lower().split())))\n",
    "\n",
    "# Output the results\n",
    "print(f\"Cosine Similarity: {cosine_sim[0][0]:.4f}\")\n",
    "print(f\"ROUGE Scores: {rouge_scores}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a1f32e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
