{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "576a67f6",
   "metadata": {},
   "source": [
    "# bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25f84d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a3f551633ea4b5593c3c2befdc38f6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80a9c63e7bf54b6193edd031c7992f35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 37.69 seconds, 0.66 sentences/sec\n",
      "Average Precision: 0.8957\n",
      "Average Recall: 0.8731\n",
      "Average F1: 0.8839\n"
     ]
    }
   ],
   "source": [
    "from bert_score import score\n",
    "import json\n",
    "\n",
    "# Load your JSON\n",
    "with open('test_questions.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract AI and GT answers\n",
    "ai_answers = [item['AI'] for item in data]\n",
    "gt_answers = [item['answer'] for item in data]\n",
    "\n",
    "# Calculate BERTScore\n",
    "P, R, F1 = score(ai_answers, gt_answers, lang=\"en\", verbose=True)\n",
    "\n",
    "# Print average\n",
    "print(f\"Average Precision: {P.mean().item():.4f}\")\n",
    "print(f\"Average Recall: {R.mean().item():.4f}\")\n",
    "print(f\"Average F1: {F1.mean().item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f6a46a",
   "metadata": {},
   "source": [
    "### The BERT exibits following result\n",
    "\n",
    "Average Precision: 0.8957\n",
    "\n",
    "Average Recall: 0.8731\n",
    "\n",
    "Average F1: 0.8839"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e30d115",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54da6d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average cosine similarity across 25 items: 0.7012\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load your JSON data\n",
    "with open('test_questions.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Load the SentenceTransformer model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Track total similarity\n",
    "total_similarity = 0.0\n",
    "num_items = len(data)\n",
    "\n",
    "# Compute cosine similarity for each item\n",
    "for item in data:\n",
    "    ai_text = item.get(\"AI\", \"\")\n",
    "    answer_text = item.get(\"answer\", \"\")\n",
    "\n",
    "    # Get embeddings\n",
    "    ai_embedding = model.encode(ai_text, convert_to_tensor=True)\n",
    "    answer_embedding = model.encode(answer_text, convert_to_tensor=True)\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    similarity = util.cos_sim(ai_embedding, answer_embedding).item()\n",
    "    total_similarity += similarity\n",
    "\n",
    "# Calculate average\n",
    "average_similarity = total_similarity / num_items\n",
    "\n",
    "print(f\"Average cosine similarity across {num_items} items: {average_similarity:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9acf6843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average BLEU Score: 0.0573\n",
      "Average ROUGE-1 F1: 0.3462\n",
      "Average ROUGE-2 F1: 0.1380\n",
      "Average ROUGE-L F1: 0.2348\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bert_score\n",
    "\n",
    "# Load data\n",
    "with open('test_questions.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Initialize variables\n",
    "bleu_scores = []\n",
    "rouge1_scores = []\n",
    "rouge2_scores = []\n",
    "rougeL_scores = []\n",
    "\n",
    "references = []\n",
    "candidates = []\n",
    "\n",
    "# Initialize scorers\n",
    "rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "smoothie = SmoothingFunction().method4\n",
    "\n",
    "# Calculate metrics\n",
    "for item in data:\n",
    "    ref = item['answer']\n",
    "    cand = item['AI']\n",
    "    \n",
    "    # BLEU\n",
    "    bleu = sentence_bleu([ref.split()], cand.split(), smoothing_function=smoothie)\n",
    "    bleu_scores.append(bleu)\n",
    "    \n",
    "    # ROUGE\n",
    "    scores = rouge.score(cand, ref)\n",
    "    rouge1_scores.append(scores['rouge1'].fmeasure)\n",
    "    rouge2_scores.append(scores['rouge2'].fmeasure)\n",
    "    rougeL_scores.append(scores['rougeL'].fmeasure)\n",
    "    \n",
    "    # For BERTScore\n",
    "    references.append(ref)\n",
    "    candidates.append(cand)\n",
    "\n",
    "\n",
    "# Averages\n",
    "avg_bleu = sum(bleu_scores) / len(bleu_scores)\n",
    "avg_rouge1 = sum(rouge1_scores) / len(rouge1_scores)\n",
    "avg_rouge2 = sum(rouge2_scores) / len(rouge2_scores)\n",
    "avg_rougeL = sum(rougeL_scores) / len(rougeL_scores)\n",
    "\n",
    "# Print results\n",
    "print(f\"Average BLEU Score: {avg_bleu:.4f}\")\n",
    "print(f\"Average ROUGE-1 F1: {avg_rouge1:.4f}\")\n",
    "print(f\"Average ROUGE-2 F1: {avg_rouge2:.4f}\")\n",
    "print(f\"Average ROUGE-L F1: {avg_rougeL:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e13355d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
